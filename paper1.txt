DrivAid: Augmenting Driving Analytics with
Multi-Modal Information
abstract :
1) limitation of conventional IMU - Inertial Measuring units
2) Audio-visual data give more accurate reslut and reason for
the cause of the event
3) The event can be due to the inattentive driving or attentive driving
4) data-> 1550 miles of the data with 4 camera(10fps) 
5) DrivAid can achive accuracy of 90% 

INTRO:
1) limitations of the IMU developed by the cambridge MObile Telematics
2) IMU can provide accrate analytics about wht happened during a drive, e.g., hard brakes,sudden lane changes etc.,
3) IMU dont provide contextual information,it doesn't provide the why driver did took that action
4) So why answer could be driver distraction or it could be to avoid an abstacle
 
Intro to DrivAid:
1) audio-visual analytics should need to be performed locally and in real-time
2) Compared to cloud computing, edge computing provides lower latency, greater responsiveness,and more efficient use of network bandwidth.
3) smartphone motion sensors to detect different driving events and only conduct further analysis once an event is detected
4) DrivAid uses data from IMU and GPS sensors to detect driving events.
5)context analysis uses blindspot monitoring and front view analysis, turn signal usage , head pose estimation
6) activity evaluation --> extracted context info --> driver behaviour good,fair,poor

Usefulness of DrivAid: 
1)  The concept of using audio-visual
cues in improved situational awareness around vehicles is most famously used in autonomous vehicle industries (along with a host of other sensors).
2) uses low resolution sensors with low cost $500 < $100000[used in autonomous vehicle sensors]
3)DrivAid predicts when accident may occur
4) many fleet operators, e.g., public transit systems, school buses, freight trucks, are often interested in understanding the behaviors of their drivers
5)Third, if certain drivers are known to be “experts,” then the annotated actions of such drivers could also be used to provide useful inputs to the design of autonomous vehicles

contributions:
1) how a real-time low-cost sensing and analysing system could be built that leverages audio-visual cues to augment driving behavior analysis based on IMU sensors in a holistic manner. 

SYSTEM OVERVIEW
A. Design Consideration:
1) existing solutions-Xsense, Vsense can accurately detect various normal and abnormal events using data collected from motion sensors or OBD port, but fail to provide surrounding context information.
2)To find the missing information, DrivAid buffers the most recent 10 seconds audio and video data from microphones and cameras for analysis. When an event of interest happens, DrivAid analyzes the data with computer vision techniques to get driving actions that cannot be detected by IMU and GPS
sensors, including turn signal usage, mirror checking, blind
spot checking, and so on. By combining information from
various sources, DrivAid can provide more informative driver
analytics and offer deeper insights into each detected events.

B. Solution used:
uses 3 types of sensors
1) IMU sensors and GPS of a phone, which is used to detect the events of driving
2) The rare camera of a phone to capture traffic ahed
3) Two cameras facing the blind spots on bothsides and 1 camera facing the driver

The timestmped data is analysed on an embedded computer deployed in the vehicle
Here they used just android phone to record data from the accelorometer, gyroscope,magnetometer and stream them to the embedded computer for the textual analysis

context info is sent to an activity evolution module, through which detected event will be evaluated and categorised into different ratings

IMU is accurate and low power consumer for detecting the events

audio-visual data analysis with deep neural network are computationally expensive

we want to conifine our study to specific events or else it consumes more time and space for computation and power consumtion also.

III)EVENT DETECTION AND CONTEXT ANALYSIS
1) The DrivAid system detects three types of driving activities (turn, hard brake, and lane change) with the sensors on the smartphone
2) The data from the microphoneshone, cameras are synchronised 

A. Driving Event Detection

1) COORDINATE ALLIGNMENT: Smartphone-based mechanisms: the vehicle’s lateral dynamics can be obtained from the gyroscope sensor in the smartphone when a phone is aligned with the vehicle

2)  Turn and Lane Change Detection: uses gyroscope and GPS of the smartphone, uses Vsense for the lane changes and driving on the curvy road

3) Brake & Stop Detection:  accelerometer readings
as well as vehicle speed information to detect brakes and stops.A hard brake is defined as any condition when the vehicle decelerates faster than 7 mph (Miles per hour) per second.

B. Driver Behavior and Context Analysis

1) Driver Behavior Analysis:  DrivAid detects turn signal usages by analyzing turn signal sounds. We apply bandpass filters to filter out human speech as well as other background noises. Then a pattern matching technique is used to identify turn signals

  DrivAid detects a driver’s head pose to infer the driver’s mirror and blind spot checking behaviors.
  
We classify head poses into six categories, they
are left and right wing mirror check, left and right blind spot check, rearview mirror check, and front view check 
Due to lack of facial features, it is hard to estimate head pose when the driver is checking right and left blind spots. To solve this issue, we track the rotation angle in a duration. If the angle keeps increasing and then disappears, then the driver is doing a right blind spot check, and vice versa
